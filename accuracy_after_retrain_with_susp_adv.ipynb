{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_nn import test_model\n",
    "from lp import run_lp\n",
    "from os import path\n",
    "from spectrum_analysis import *\n",
    "from utils import save_perturbed_test_groups, load_perturbed_test_groups\n",
    "from utils import load_suspicious_neurons, save_suspicious_neurons\n",
    "from utils import create_experiment_dir, get_trainable_layers\n",
    "from utils import load_classifications, save_classifications\n",
    "from utils import save_layer_outs, load_layer_outs, construct_spectrum_matrices\n",
    "from utils import load_MNIST, load_CIFAR, load_model\n",
    "from utils import filter_val_set, save_original_inputs\n",
    "from input_synthesis import synthesize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = \"experiment_results\"\n",
    "model_path = \"neural_networks\"\n",
    "group_index = 1\n",
    "__version__ = \"v1.0\"\n",
    "\n",
    "\n",
    "nb_classes = 10\n",
    "perturbed_count = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args =  {\n",
    "    'model': 'mnist_test_model_5_30_relu',\n",
    "    'suspicious_num': 10\n",
    "}\n",
    "\n",
    "args = defaultdict(lambda: None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name     = args['model']\n",
    "dataset        = args['dataset'] if not args['dataset'] == None else 'mnist'\n",
    "step_size      = args['step_size'] if not args['step_size'] == None else 1\n",
    "distance       = args['distance'] if not args['distance'] ==None else 0.1\n",
    "approach       = args['approach'] if not args['approach'] == None else 'tarantula'\n",
    "susp_num       = args['suspicious_num'] if not args['suspicious_num'] == None else 1\n",
    "repeat         = args['repeat'] if not args['repeat'] == None else 1\n",
    "seed           = args['seed'] if not args['seed'] == None else random.randint(0,10)\n",
    "star           = args['star'] if not args['star'] == None else 3\n",
    "logfile_name   = args['logfile'] if not args['logfile'] == None else 'result.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# 0) Load MNIST or CIFAR10 data\n",
    "if dataset == 'mnist':\n",
    "    X_train, Y_train, X_test, Y_test = load_MNIST()\n",
    "else:\n",
    "    X_train, Y_train, X_test, Y_test = load_CIFAR()\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n",
    "                                                  test_size=1/6.0,\n",
    "                                                  random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# 1) Load the pretrained network.\n",
    "try:\n",
    "    model = load_model(path.join(model_path, model_name))\n",
    "except:\n",
    "    logfile.write(\"Model not found! Provide a pre-trained model model as input.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(selected_class, X, Y):\n",
    "\n",
    "    logfile = open(logfile_name, 'a')\n",
    "\n",
    "    #Fault localization is done per class.\n",
    "    X, Y = filter_val_set(selected_class, X, Y)\n",
    "\n",
    "    ####################\n",
    "    # 2)test the model and receive the indexes of correct and incorrect classifications\n",
    "    # Also provide output of each neuron in each layer for test input x.\n",
    "    correct_classifications, misclassifications, layer_outs, predictions = test_model(model, X, Y)\n",
    "\n",
    "    ####################\n",
    "    # 3) Receive the correct classifications  & misclassifications and identify \n",
    "    # the suspicious neurons per layer\n",
    "    trainable_layers = get_trainable_layers(model)\n",
    "    scores, num_cf, num_uf, num_cs, num_us = construct_spectrum_matrices(model,\n",
    "                                                                        trainable_layers,\n",
    "                                                                        correct_classifications,\n",
    "                                                                        misclassifications,\n",
    "                                                                        layer_outs)\n",
    "\n",
    "    filename = experiment_path + '/' + model_name + '_C' + str(selected_class) + '_' +\\\n",
    "    approach +  '_SN' +  str(susp_num)\n",
    "\n",
    "    if approach == 'tarantula':\n",
    "        suspicious_neuron_idx = tarantula_analysis(trainable_layers, scores,\n",
    "                                             num_cf, num_uf, num_cs, num_us,\n",
    "                                             susp_num)\n",
    "    else:\n",
    "        print('Wrong approach')\n",
    "        exit()\n",
    "\n",
    "\n",
    "    logfile.write('Suspicous neurons: ' + str(suspicious_neuron_idx) + '\\n')\n",
    "\n",
    "    ####################\n",
    "    # 4) Run Suspiciousness-Guided Input Synthesis Algorithm\n",
    "    # Receive the set of suspicious neurons for each layer from Step 3 # and \n",
    "    # will produce new inputs based on the correct classifications (from the \n",
    "    # testing set) that exercise the suspicious neurons\n",
    "\n",
    "    perturbed_xs = []\n",
    "    perturbed_ys = []\n",
    "\n",
    "    # select 10 inputs randomly from the correct classification set.\n",
    "    #selected = list(correct_classifications)\n",
    "    if perturbed_count == -1:\n",
    "        selected = list(correct_classifications)\n",
    "    else:\n",
    "        selected = np.random.choice(list(correct_classifications), size=perturbed_count, replace=False)\n",
    "        \n",
    "    zipped_data = zip(list(np.array(X)[selected]), list(np.array(Y)[selected]))\n",
    "\n",
    "    syn_start = datetime.datetime.now()\n",
    "    x_perturbed, y_perturbed, x_original = synthesize(model, zipped_data,\n",
    "                                           suspicious_neuron_idx,\n",
    "                                           correct_classifications,\n",
    "                                           step_size,\n",
    "                                           distance)\n",
    "    syn_end = datetime.datetime.now()\n",
    "\n",
    "    perturbed_xs = perturbed_xs + x_perturbed\n",
    "    perturbed_ys = perturbed_ys + y_perturbed\n",
    "\n",
    "    # reshape them into the expected format\n",
    "    perturbed_xs = np.asarray(perturbed_xs).reshape(np.asarray(perturbed_xs).shape[0],\n",
    "                                     *X[0].shape)\n",
    " \n",
    "    perturbed_ys = np.asarray(perturbed_ys).reshape(np.asarray(perturbed_ys).shape[0], 10)\n",
    "\n",
    "    for i in range(len(perturbed_xs)):\n",
    "        name = 'susp_adv_inputs/'+model_name+'_'+str(perturbed_ys[i])+'_C'+str(selected_class)+'_susp_guided_'+str(selected[i])+'.png'\n",
    "        plt.imsave(name, perturbed_xs[i].reshape(28,28), cmap='gray')\n",
    "\n",
    "    ####################\n",
    "    # 5) Test if the mutated inputs are adversarial\n",
    "    score = model.evaluate(perturbed_xs, perturbed_ys, verbose=0)\n",
    "    logfile.write('Model: ' + model_name + ', Class: ' + str(selected_class) +\n",
    "                  ', Approach: ' + approach + ', Distance: ' +\n",
    "                  str(distance) + ', Score: ' + str(score) + '\\n')\n",
    "\n",
    "    logfile.write('Input Synthesis Time: ' + str(syn_end-syn_start) + '\\n')\n",
    "\n",
    "    logfile.close()\n",
    "    \n",
    "    return perturbed_xs, perturbed_ys, suspicious_neuron_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_xs_by_class, perturbed_ys_by_class, sus_ids_by_class = range(nb_classes), range(nb_classes), range(nb_classes)\n",
    "\n",
    "for selected_class in range(nb_classes):\n",
    "    perturbed_xs_by_class[selected_class], perturbed_ys_by_class[selected_class], sus_ids_by_class[selected_class] = exp(selected_class, X_val, Y_val)\n",
    "\n",
    "for selected_class in range(nb_classes):\n",
    "    model.fit(perturbed_xs_by_class[selected_class], perturbed_ys_by_class[selected_class], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.concatenate(perturbed_xs_by_class)\n",
    "a=np.concatenate([a, X_train])\n",
    "b=np.concatenate(perturbed_ys_by_class)\n",
    "b=np.concatenate([b, Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = models.model_from_json(model.to_json())\n",
    "new_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "new_model.fit(a, b, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrained_metrics = new_model.evaluate(X_test, Y_test)\n",
    "print(retrained_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env_27",
   "language": "python",
   "name": "tensorflow_env_27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
