{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from test_nn import test_model\n",
    "from lp import run_lp\n",
    "from os import path\n",
    "from spectrum_analysis import *\n",
    "from utils import save_perturbed_test_groups, load_perturbed_test_groups\n",
    "from utils import load_suspicious_neurons, save_suspicious_neurons\n",
    "from utils import create_experiment_dir, get_trainable_layers\n",
    "from utils import load_classifications, save_classifications\n",
    "from utils import save_layer_outs, load_layer_outs, construct_spectrum_matrices\n",
    "from utils import load_MNIST, load_CIFAR, load_model\n",
    "from utils import filter_val_set, save_original_inputs\n",
    "from input_synthesis import synthesize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = \"experiment_results\"\n",
    "model_path = \"neural_networks\"\n",
    "group_index = 1\n",
    "__version__ = \"v1.0\"\n",
    "\n",
    "\n",
    "nb_classes = 10\n",
    "perturbed_count = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args =  {\n",
    "    'model': 'mnist_test_model_5_30_relu',\n",
    "    'suspicious_num': 10\n",
    "}\n",
    "\n",
    "args = defaultdict(lambda: None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name     = args['model']\n",
    "dataset        = args['dataset'] if not args['dataset'] == None else 'mnist'\n",
    "step_size      = args['step_size'] if not args['step_size'] == None else 1\n",
    "distance       = args['distance'] if not args['distance'] ==None else 0.1\n",
    "approach       = args['approach'] if not args['approach'] == None else 'tarantula'\n",
    "susp_num       = args['suspicious_num'] if not args['suspicious_num'] == None else 1\n",
    "repeat         = args['repeat'] if not args['repeat'] == None else 1\n",
    "seed           = args['seed'] if not args['seed'] == None else random.randint(0,10)\n",
    "star           = args['star'] if not args['star'] == None else 3\n",
    "logfile_name   = args['logfile'] if not args['logfile'] == None else 'result.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# 0) Load MNIST or CIFAR10 data\n",
    "if dataset == 'mnist':\n",
    "    X_train, Y_train, X_test, Y_test = load_MNIST()\n",
    "else:\n",
    "    X_train, Y_train, X_test, Y_test = load_CIFAR()\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n",
    "                                                  test_size=1/6.0,\n",
    "                                                  random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Model structure loaded from ', 'neural_networks/mnist_test_model_5_30_relu')\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# 1) Load the pretrained network.\n",
    "try:\n",
    "    model = load_model(path.join(model_path, model_name))\n",
    "except:\n",
    "    logfile.write(\"Model not found! Provide a pre-trained model model as input.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(selected_class):\n",
    "\n",
    "    logfile = open(logfile_name, 'a')\n",
    "\n",
    "    experiment_name = create_experiment_dir(experiment_path, model_name,\n",
    "                                            selected_class, step_size,\n",
    "                                            approach, susp_num, repeat)\n",
    "\n",
    "    #Fault localization is done per class.\n",
    "    X_val, Y_val = filter_val_set(selected_class, X_test, Y_test)\n",
    "\n",
    "\n",
    "    ####################\n",
    "    # 2)test the model and receive the indexes of correct and incorrect classifications\n",
    "    # Also provide output of each neuron in each layer for test input x.\n",
    "    filename = experiment_path + '/' + model_name + '_' + str(selected_class)\n",
    "    try:\n",
    "        correct_classifications, misclassifications = load_classifications(filename, group_index)\n",
    "        layer_outs = load_layer_outs(filename, group_index)\n",
    "    except:\n",
    "        correct_classifications, misclassifications, layer_outs, predictions =\\\n",
    "                test_model(model, X_val, Y_val)\n",
    "        save_classifications(correct_classifications, misclassifications,\n",
    "                             filename, group_index)\n",
    "        save_layer_outs(layer_outs, filename, group_index)\n",
    "\n",
    "\n",
    "    ####################\n",
    "    # 3) Receive the correct classifications  & misclassifications and identify \n",
    "    # the suspicious neurons per layer\n",
    "    trainable_layers = get_trainable_layers(model)\n",
    "    scores, num_cf, num_uf, num_cs, num_us = construct_spectrum_matrices(model,\n",
    "                                                                        trainable_layers,\n",
    "                                                                        correct_classifications,\n",
    "                                                                        misclassifications,\n",
    "                                                                        layer_outs)\n",
    "\n",
    "    filename = experiment_path + '/' + model_name + '_C' + str(selected_class) + '_' +\\\n",
    "    approach +  '_SN' +  str(susp_num)\n",
    "\n",
    "    if approach == 'tarantula':\n",
    "        try:\n",
    "            suspicious_neuron_idx = load_suspicious_neurons(filename, group_index)\n",
    "        except:\n",
    "            suspicious_neuron_idx = tarantula_analysis(trainable_layers, scores,\n",
    "                                                 num_cf, num_uf, num_cs, num_us,\n",
    "                                                 susp_num)\n",
    "\n",
    "            save_suspicious_neurons(suspicious_neuron_idx, filename, group_index)\n",
    "\n",
    "    elif approach == 'ochiai':\n",
    "        try:\n",
    "            suspicious_neuron_idx = load_suspicious_neurons(filename, group_index)\n",
    "        except:\n",
    "            suspicious_neuron_idx = ochiai_analysis(trainable_layers, scores,\n",
    "                                                 num_cf, num_uf, num_cs, num_us,\n",
    "                                                 susp_num)\n",
    "\n",
    "            save_suspicious_neurons(suspicious_neuron_idx, filename, group_index)\n",
    "\n",
    "    elif approach == 'dstar':\n",
    "        try:\n",
    "            suspicious_neuron_idx = load_suspicious_neurons(filename, group_index)\n",
    "        except:\n",
    "            suspicious_neuron_idx = dstar_analysis(trainable_layers, scores,\n",
    "                                                 num_cf, num_uf, num_cs, num_us,\n",
    "                                                 susp_num, star)\n",
    "\n",
    "            save_suspicious_neurons(suspicious_neuron_idx, filename, group_index)\n",
    "\n",
    "    elif approach == 'random':\n",
    "        # Random fault localization has to be run after running Tarantula,\n",
    "        # Ochiai and DStar with the same parameters.\n",
    "\n",
    "        filename = experiment_path + '/' + model_name + '_C' + str(selected_class) \\\n",
    "        + '_tarantula_' + 'SN' + str(susp_num)\n",
    "\n",
    "        suspicious_neuron_idx_tarantula = load_suspicious_neurons(filename, group_index)\n",
    "\n",
    "        filename = experiment_path + '/' + model_name + '_C' + str(selected_class) \\\n",
    "        + '_ochiai_' + 'SN' + str(susp_num)\n",
    "\n",
    "        suspicious_neuron_idx_ochiai = load_suspicious_neurons(filename, group_index)\n",
    "\n",
    "        filename = experiment_path + '/' + model_name + '_C' + str(selected_class) \\\n",
    "        + '_dstar_' + 'SN' + str(susp_num)\n",
    "\n",
    "        suspicious_neuron_idx_dstar = load_suspicious_neurons(filename, group_index)\n",
    "\n",
    "        forbiddens = suspicious_neuron_idx_ochiai + suspicious_neuron_idx_tarantula  + \\\n",
    "        suspicious_neuron_idx_dstar\n",
    "\n",
    "        forbiddens = [list(forb) for forb in forbiddens]\n",
    "\n",
    "        available_layers = list(([elem[0] for elem in suspicious_neuron_idx_tarantula]))\n",
    "        available_layers += list(set([elem[0] for elem in suspicious_neuron_idx_ochiai]))\n",
    "        available_layers += list(set([elem[0] for elem in suspicious_neuron_idx_dstar]))\n",
    "\n",
    "        suspicious_neuron_idx = []\n",
    "        while len(suspicious_neuron_idx) < susp_num:\n",
    "            l_idx = random.choice(available_layers)\n",
    "            n_idx = random.choice(range(model.layers[l_idx].output_shape[1]))\n",
    "\n",
    "            if [l_idx, n_idx] not in forbiddens and [l_idx, n_idx] not in suspicious_neuron_idx:\n",
    "                suspicious_neuron_idx.append([l_idx, n_idx])\n",
    "\n",
    "\n",
    "    logfile.write('Suspicous neurons: ' + str(suspicious_neuron_idx) + '\\n')\n",
    "\n",
    "    ####################\n",
    "    # 4) Run Suspiciousness-Guided Input Synthesis Algorithm\n",
    "    # Receive the set of suspicious neurons for each layer from Step 3 # and \n",
    "    # will produce new inputs based on the correct classifications (from the \n",
    "    # testing set) that exercise the suspicious neurons\n",
    "\n",
    "    perturbed_xs = []\n",
    "    perturbed_ys = []\n",
    "\n",
    "    # select 10 inputs randomly from the correct classification set.\n",
    "    #selected = list(correct_classifications)\n",
    "    if perturbed_count == -1:\n",
    "        selected = list(correct_classifications)\n",
    "    else:\n",
    "        selected = np.random.choice(list(correct_classifications), perturbed_count)\n",
    "        \n",
    "    zipped_data = zip(list(np.array(X_val)[selected]), list(np.array(Y_val)[selected]))\n",
    "\n",
    "    syn_start = datetime.datetime.now()\n",
    "    x_perturbed, y_perturbed, x_original = synthesize(model, zipped_data,\n",
    "                                           suspicious_neuron_idx,\n",
    "                                           correct_classifications,\n",
    "                                           step_size,\n",
    "                                           distance)\n",
    "    syn_end = datetime.datetime.now()\n",
    "\n",
    "    perturbed_xs = perturbed_xs + x_perturbed\n",
    "    perturbed_ys = perturbed_ys + y_perturbed\n",
    "\n",
    "    # reshape them into the expected format\n",
    "    perturbed_xs = np.asarray(perturbed_xs).reshape(np.asarray(perturbed_xs).shape[0],\n",
    "                                     *X_val[0].shape)\n",
    " \n",
    "    perturbed_ys = np.asarray(perturbed_ys).reshape(np.asarray(perturbed_ys).shape[0], 10)\n",
    "\n",
    "    #save perturtbed inputs\n",
    "    filename = path.join(experiment_path, experiment_name)\n",
    "    try:\n",
    "        save_perturbed_test_groups(perturbed_xs, perturbed_ys, filename, group_index)\n",
    "        save_original_inputs(x_original, filename, group_index)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    ####################\n",
    "    # 5) Test if the mutated inputs are adversarial\n",
    "    score = model.evaluate(perturbed_xs, perturbed_ys, verbose=0)\n",
    "    logfile.write('Model: ' + model_name + ', Class: ' + str(selected_class) +\n",
    "                  ', Approach: ' + approach + ', Distance: ' +\n",
    "                  str(distance) + ', Score: ' + str(score) + '\\n')\n",
    "\n",
    "    logfile.write('Input Synthesis Time: ' + str(syn_end-syn_start) + '\\n')\n",
    "\n",
    "    logfile.close()\n",
    "    \n",
    "    return perturbed_xs, perturbed_ys, suspicious_neuron_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCurrently not available\\n####################\\n# 6) retrain the model\\n# train_model_fault_localisation(model, x_perturbed, y_perturbed, len(x_perturbed))\\nmodel.fit(x_perturbed, y_perturbed, batch_size=32, epochs=10, verbose=1)\\n\\n####################\\n# 7) retest the model\\ntest_model(model, X_test, Y_test)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Currently not available\n",
    "####################\n",
    "# 6) retrain the model\n",
    "# train_model_fault_localisation(model, x_perturbed, y_perturbed, len(x_perturbed))\n",
    "model.fit(x_perturbed, y_perturbed, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "####################\n",
    "# 7) retest the model\n",
    "test_model(model, X_test, Y_test)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 136us/step\n"
     ]
    }
   ],
   "source": [
    "initial = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set filtered for desired class: 0\n",
      "experiment_results/mnist_test_model_5_30_relu_0_classifications.h5\n",
      "('Classifications loaded from ', 'experiment_results/mnist_test_model_5_30_relu_0_classifications.h5')\n",
      "('Layer outs loaded from ', 'experiment_results/mnist_test_model_5_30_relu_0_layer_outs.h5')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Suspicious neurons  loaded from ', 'experiment_results/mnist_test_model_5_30_relu_C0_tarantula_SN10_suspicious_neurons.h5')\n",
      "('Input index:', 10)\n",
      "('Input index:', 20)\n",
      "('Input index:', 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/inspect.py\", line 1051, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/inspect.py\", line 1011, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/inspect.py\", line 499, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/posixpath.py\", line 375, in realpath\n",
      "    path, ok = _joinrealpath('', filename, {})\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/posixpath.py\", line 399, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/anaconda3/envs/tensorflow_env_p27/lib/python2.7/posixpath.py\", line 61, in join\n",
      "    def join(a, *p):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env_p27/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "perturbed_xs_by_class, perturbed_ys_by_class, sus_ids_by_class = range(nb_classes), range(nb_classes), range(nb_classes)\n",
    "\n",
    "for selected_class in range(nb_classes):\n",
    "    perturbed_xs_by_class[selected_class], perturbed_ys_by_class[selected_class], sus_ids_by_class[selected_class] = exp(selected_class)\n",
    "\n",
    "for selected_class in range(nb_classes):\n",
    "    model.fit(perturbed_xs_by_class[selected_class], perturbed_ys_by_class[selected_class], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.concatenate(perturbed_xs_by_class)\n",
    "a=np.concatenate([a, X_train])\n",
    "b=np.concatenate(perturbed_ys_by_class)\n",
    "b=np.concatenate([b, Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = models.model_from_json(model.to_json())\n",
    "new_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "new_model.fit(a, b, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrained_metrics = new_model.evaluate(X_test, Y_test)\n",
    "print(retrained_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env_27",
   "language": "python",
   "name": "tensorflow_env_27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
